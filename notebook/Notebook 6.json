{
	"name": "Notebook 6",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6c73937c-b8f0-423a-85de-ed7e2dc964b8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/35c7c256-febc-4c53-82a5-71e8573a3c24/resourceGroups/rg-pmishra-training-001/providers/Microsoft.Synapse/workspaces/trainparmish/bigDataPools/Sparkpool",
				"name": "Sparkpool",
				"type": "Spark",
				"endpoint": "https://trainparmish.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Pax"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"# Initialize Spark Session\r\n",
					"spark = SparkSession.builder.getOrCreate()\r\n",
					"\r\n",
					"# Azure Blob Storage details\r\n",
					"blob_account_name = \"synapsesazerog\"\r\n",
					"blob_container_name = \"raw\"\r\n",
					"\r\n",
					"# Set the SAS token for accessing the Blob Storage\r\n",
					"sas_token = \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-10-07T02:05:04Z&st=2024-10-03T18:05:04Z&spr=https,http&sig=yTDeBjWXfWz%2BRNUUnIuWB%2BrliCkrpboOQZ30UOatUEc%3D\"  # Replace with the actual SAS token\r\n",
					"\r\n",
					"# Configure Spark with the SAS token\r\n",
					"spark.conf.set(\r\n",
					"    f'fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net',\r\n",
					"    sas_token\r\n",
					")\r\n",
					"\r\n",
					"\r\n",
					"# Load CSV data from Blob Storage\r\n",
					"DF_PAX = spark.read.format('csv')\\\r\n",
					"                   .option('header', 'true')\\\r\n",
					"                   .option('inferSchema', 'true')\\\r\n",
					"                   .load(f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/stage/*')\r\n",
					"\r\n",
					"# Create a temporary view for SQL queries\r\n",
					"DF_PAX.createOrReplaceTempView(\"TempView_pax\")\r\n",
					"\r\n",
					"# Execute SQL query on the temp view\r\n",
					"DF_PAX = spark.sql(\"SELECT * FROM TempView_pax\")\r\n",
					"\r\n",
					"# DF_PAX.printSchema()"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Sales"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"DF_Sales= spark.read.format('csv')\\\r\n",
					"                        .option('header', 'true')\\\r\n",
					"                        .option('inferred','true')\\\r\n",
					"                        .load('wasbs://raw@synapsesazerog.blob.core.windows.net/stage/sales/*')\r\n",
					"\r\n",
					"\r\n",
					"display(DF_Sales)\r\n",
					"\r\n",
					"DF_Sales.createOrReplaceTempView(\"TempView_sales\")\r\n",
					"\r\n",
					"# if Debug:\r\n",
					"#     display(DF.limit(NoOfRecordsToDisplay)) \r\n",
					"\r\n",
					"DF_Sales = spark.sql(\"SELECT * FROM TempView_sales\")\r\n",
					"\r\n",
					"DF_Sales.printSchema()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Loading"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"DF_loading= spark.read.format('json')\\\r\n",
					"                        .option('header', 'true')\\\r\n",
					"                        .option('inferred','true')\\\r\n",
					"                        .load('wasbs://raw@synapsesazerog.blob.core.windows.net/loading/*')\r\n",
					"\r\n",
					"\r\n",
					"display(DF_loading)\r\n",
					"\r\n",
					"\r\n",
					"DF_loading.createOrReplaceTempView(\"TempView_loading\")\r\n",
					"\r\n",
					"# if Debug:\r\n",
					"#     display(DF.limit(NoOfRecordsToDisplay)) \r\n",
					"\r\n",
					"DF_loading = spark.sql(\"SELECT * FROM TempView_loading\")\r\n",
					"\r\n",
					"# DF_Sales.printSchema()\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.functions import col, count, sum, avg, to_date, when\r\n",
					"\r\n",
					"# Assume spark session is already created\r\n",
					"\r\n",
					"# Load data from temp views\r\n",
					"df_sales = spark.sql(\"SELECT * FROM TempView_sales\")\r\n",
					"df_loading = spark.sql(\"SELECT * FROM TempView_loading\")\r\n",
					"df_pax = spark.sql(\"SELECT * FROM TempView_pax\")\r\n",
					"\r\n",
					"def process_data(df, is_loading=False):\r\n",
					"    # Convert relevant columns to appropriate types\r\n",
					"    df_processed = df.withColumn(\"Flight Date\", to_date(col(\"Flight Date\"))) \\\r\n",
					"                     .withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\")) \\\r\n",
					"                     .withColumn(\"NetSales\", col(\"NetSales\").cast(\"double\")) \\\r\n",
					"                     .withColumn(\"PassengerCount\", col(\"PassengerCount\").cast(\"integer\"))\r\n",
					"\r\n",
					"    # Aggregate data per flight\r\n",
					"    agg_cols = {\r\n",
					"        \"Quantity\": \"sum\",\r\n",
					"        \"NetSales\": \"sum\",\r\n",
					"        \"PassengerCount\": \"max\",\r\n",
					"        \"TransactionID\": \"count\"\r\n",
					"    }\r\n",
					"    \r\n",
					"    if is_loading:\r\n",
					"        agg_cols[\"Quantity\"] = \"sum\"\r\n",
					"    \r\n",
					"    df_aggregated = df_processed.groupBy(\"Flight\", \"Flight Date\") \\\r\n",
					"        .agg(\r\n",
					"            *[getattr(func(col(c)), \"alias\")(f\"{c}_{'loaded' if is_loading else 'sold'}\")\r\n",
					"              for c, func in agg_cols.items()]\r\n",
					"        )\r\n",
					"\r\n",
					"    return df_aggregated\r\n",
					"\r\n",
					"# Process sales and loading data\r\n",
					"df_sales_agg = process_data(df_sales)\r\n",
					"df_loading_agg = process_data(df_loading, is_loading=True)\r\n",
					"\r\n",
					"# Combine sales and loading data\r\n",
					"df_combined = df_sales_agg.join(df_loading_agg, [\"Flight\", \"Flight Date\"], \"outer\")\r\n",
					"\r\n",
					"# Add pax data if it exists and has a different schema\r\n",
					"if df_pax is not None and df_pax.schema != df_sales.schema:\r\n",
					"    df_pax_processed = df_pax.withColumn(\"Flight Date\", to_date(col(\"Flight Date\")))\r\n",
					"    df_combined = df_combined.join(df_pax_processed, [\"Flight\", \"Flight Date\"], \"left\")\r\n",
					"\r\n",
					"# Data for Data Scientists\r\n",
					"def prepare_data_for_scientists(df):\r\n",
					"    return df.select(\"Flight\", \"Flight Date\", \"PassengerCount_sold\", \"Quantity_sold\")\r\n",
					"\r\n",
					"# Data for Data Analysts\r\n",
					"def prepare_data_for_analysts(df):\r\n",
					"    return df\r\n",
					"\r\n",
					"# Execute the pipeline\r\n",
					"df_for_scientists = prepare_data_for_scientists(df_combined)\r\n",
					"df_for_analysts = prepare_data_for_analysts(df_combined)\r\n",
					"\r\n",
					"# Save the results\r\n",
					"df_for_scientists.write.mode(\"overwrite\").parquet(\"abfss://processed@synapsesazerog.dfs.core.windows.net/data_for_scientists/\")\r\n",
					"df_for_analysts.write.mode(\"overwrite\").parquet(\"abfss://processed@synapsesazerog.dfs.core.windows.net/data_for_analysts/\")\r\n",
					"\r\n",
					"print(\"Data processing complete. Results saved to Azure Data Lake Storage Gen2.\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.functions import col, count, sum, avg, to_date, when, max as max_agg\r\n",
					"\r\n",
					"# Assume spark session is already created\r\n",
					"\r\n",
					"# Load data from temp views\r\n",
					"df_sales = spark.sql(\"SELECT * FROM TempView_sales\")\r\n",
					"df_loading = spark.sql(\"SELECT * FROM TempView_loading\")\r\n",
					"df_pax = spark.sql(\"SELECT * FROM TempView_pax\")\r\n",
					"\r\n",
					"\r\n",
					"display(df_sales)\r\n",
					"\r\n",
					"# def process_data(df, is_loading=False):\r\n",
					"#     # Convert relevant columns to appropriate types\r\n",
					"#     df_processed = df.withColumn(\"Flight Date\", to_date(col(\"Flight Date\"))) \\\r\n",
					"#                      .withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\")) \\\r\n",
					"#                      .withColumn(\"NetSales\", col(\"NetSales\").cast(\"double\")) \\\r\n",
					"#                      .withColumn(\"PassengerCount\", col(\"PassengerCount\").cast(\"integer\"))\r\n",
					"\r\n",
					"#     # Aggregate data per flight\r\n",
					"#     agg_exprs = [\r\n",
					"#         sum(col(\"Quantity\")).alias(f\"Quantity_{'loaded' if is_loading else 'sold'}\"),\r\n",
					"#         sum(col(\"NetSales\")).alias(f\"NetSales_{'loaded' if is_loading else 'sold'}\"),\r\n",
					"#         max_agg(col(\"PassengerCount\")).alias(f\"PassengerCount_{'loaded' if is_loading else 'sold'}\"),\r\n",
					"#         count(col(\"TransactionID\")).alias(f\"TransactionID_{'loaded' if is_loading else 'sold'}\")\r\n",
					"#     ]\r\n",
					"    \r\n",
					"#     df_aggregated = df_processed.groupBy(\"Flight\", \"Flight Date\").agg(*agg_exprs)\r\n",
					"\r\n",
					"#     return df_aggregated\r\n",
					"\r\n",
					"# # Process sales and loading data\r\n",
					"# df_sales_agg = process_data(df_sales)\r\n",
					"# df_loading_agg = process_data(df_loading, is_loading=True)\r\n",
					"\r\n",
					"# # Combine sales and loading data\r\n",
					"# df_combined = df_sales_agg.join(df_loading_agg, [\"Flight\", \"Flight Date\"], \"outer\")\r\n",
					"\r\n",
					"# # Add pax data if it exists\r\n",
					"# if df_pax is not None:\r\n",
					"#     df_pax_processed = df_pax.withColumn(\"Flight Date\", to_date(col(\"Flight Date\")))\r\n",
					"#     df_combined = df_combined.join(df_pax_processed, [\"Flight\", \"Flight Date\"], \"left\")\r\n",
					"\r\n",
					"# # Data for Data Scientists\r\n",
					"# def prepare_data_for_scientists(df):\r\n",
					"#     return df.select(\"Flight\", \"Flight Date\", \"PassengerCount_sold\", \"Quantity_sold\")\r\n",
					"\r\n",
					"# # Data for Data Analysts\r\n",
					"# def prepare_data_for_analysts(df):\r\n",
					"#     return df\r\n",
					"\r\n",
					"# # Execute the pipeline\r\n",
					"# df_for_scientists = prepare_data_for_scientists(df_combined)\r\n",
					"# df_for_analysts = prepare_data_for_analysts(df_combined)\r\n",
					"\r\n",
					"# # Save the results\r\n",
					"# df_for_scientists.write.mode(\"overwrite\").parquet(\"abfss://processed@synapsesazerog.dfs.core.windows.net/data_for_scientists/\")\r\n",
					"# df_for_analysts.write.mode(\"overwrite\").parquet(\"abfss://processed@synapsesazerog.dfs.core.windows.net/data_for_analysts/\")\r\n",
					"\r\n",
					"print(\"Data processing complete. Results saved to Azure Data Lake Storage Gen2.\")"
				],
				"execution_count": 32
			}
		]
	}
}
{
	"name": "NB_Cleanse_Layer_Full_Load_Intial",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 3,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "3",
				"spark.dynamicAllocation.maxExecutors": "3",
				"spark.autotune.trackingId": "5a2ae221-a0df-4ee2-bdff-6a38246f2f1f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/c67909d9-1ec0-4fbc-b557-2ec605e21fdf/resourceGroups/rg-stericycledataplatform-sandbox-001/providers/Microsoft.Synapse/workspaces/azsandboxweusynapse001/bigDataPools/sparkPool",
				"name": "sparkPool",
				"type": null,
				"endpoint": null,
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": null,
				"nodeCount": 0,
				"cores": 0,
				"memory": 0,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# <h1><b><center>Raw To Cleanse Layer</center></b></h1>\r\n",
					"\r\n",
					"## Initialise"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"# spark = SparkSession.builder \\\r\n",
					"#     .appName(\"MySparkJob\") \\\r\n",
					"#     .config(\"spark.executor.cores\", \"12\") \\\r\n",
					"#     .getOrCreate()\r\n",
					"\r\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql import Row\r\n",
					"from pyspark.sql.types import StringType,StructType\r\n",
					"from datetime import *\r\n",
					"from delta.tables import *\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"import pandas as pd\r\n",
					"import pyspark.sql.functions as F\r\n",
					"from datetime import datetime\r\n",
					"from pyspark.sql.functions import md5, col, when, lit\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from pyspark.sql.functions import lag\r\n",
					"from pyspark.sql.functions import row_number\r\n",
					"from pyspark.sql.functions import current_timestamp\r\n",
					"\r\n",
					"\r\n",
					"#Debug\r\n",
					"Debug = True\r\n",
					"NoOfRecordsToDisplay = 1000\r\n",
					"\r\n",
					"#Set Spark Config\r\n",
					"spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\r\n",
					"spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\r\n",
					"spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\r\n",
					"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",10485760)\r\n",
					"spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
					"spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
					"spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism * 2)\r\n",
					"#Config needed to read datetime column values without error [Don't remove]\r\n",
					"spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
					"spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
					"spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
					"spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# #Parameters\r\n",
					"# RawContainerName = 'rawlayer'\r\n",
					"# RawStorageAccountName = 'azsandboxweudlslake001'\r\n",
					"# SourceSystemName = 'AX'\r\n",
					"# RawRelativePath = '/Inbound/'\r\n",
					"# FolderName = 'LogisticsLocation'\r\n",
					"# CleanseContainerName = 'cleansedlayer'\r\n",
					"# CleanseStorageAccountName = 'azsandboxweudlslake001'\r\n",
					"# CleanRelativePath = '/Clean/'\r\n",
					"# HistoryRelativePath = '/History/'\r\n",
					"# All_Source_Column_Names = 'RECID,CREATEDDATETIME,DESCRIPTION,DUNSNUMBERRECID,ISPOSTALADDRESS,LOCATIONID,MODIFIEDBY,MODIFIEDDATETIME,PARENTLOCATION,PARTITION,RECVERSION'\r\n",
					"# SourceKeysWithNotNullCheck = 'RECID'\r\n",
					"# # SourceKeysWithNotNullCheck = 'RECID'\r\n",
					"# SourceSystem = 'SourceSystem'\r\n",
					"# LND_File_Timestamp ='2023-08-22 12:56:54.000'\r\n",
					"# Raw_File_Name = 'LogisticsLocation_Data_2023.15.04_10.15.10_1.parquet'\r\n",
					"# #Raw_File_Name =  'LogisticsLocation_Data_2023.08.22_02.39.10_1.parquet'\r\n",
					"# #Need to pass#\r\n",
					"# CleanseContainerName ='cleansedlayer'\r\n",
					"# LND_File_Name_Pattern = 'LogisticsLocation_data'\r\n",
					"# # Create_DTTM = '2023-08-31 07:12:58.860'\r\n",
					"# # Update_DTTM = '2023-08-31 07:12:58.860'\r\n",
					""
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# #Parameters\r\n",
					"# #Upsert_Keys = ''\r\n",
					"# Raw_Inbound_Path =''\r\n",
					"# Raw_File_Name=''\r\n",
					"# Raw_Outbound_Path=''\r\n",
					"# Src_File_Timestamp  = ''\r\n",
					"# Partition_Keys =''\r\n",
					"# RawContainerName=''\r\n",
					"# Non_key_columns=''\r\n",
					"# RawStorageAccountName=''\r\n",
					"# SourceSystemName=''\r\n",
					"# RawRelativePath=''\r\n",
					"# FolderName=''\r\n",
					"# CleanseContainerName=''\r\n",
					"# CleanseStorageAccountName=''\r\n",
					"# CleanRelativePath=''\r\n",
					"# HistoryRelativePath=''\r\n",
					"# SourceColumnNames =''\r\n",
					"# SourceKeysWithNotNullCheck=''\r\n",
					"# Raw_File_Name =''\r\n",
					"# SourceSystem = ''\r\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Parameters\r\n",
					"# print(Upsert_Keys) \r\n",
					"# print(Raw_Inbound_Path) \r\n",
					"# print(Raw_File_Name) \r\n",
					"# print(Raw_Outbound_Path)\r\n",
					"# print(Partition_Keys)\r\n",
					"# print(RawContainerName)\r\n",
					"# print(Non_key_columns)\r\n",
					"# print(HistoryRelativePath)\r\n",
					"# print(CleanRelativePath)\r\n",
					"# print(CleanseContainerName)\r\n",
					"# print(RawStorageAccountName)\r\n",
					"# print(RawRelativePath)\r\n",
					"\r\n",
					"# RawRelativePath \r\n",
					"# FolderName \r\n",
					"# CleanseContainerName \r\n",
					"# CleanseStorageAccountName \r\n",
					"# CleanRelativePath \r\n",
					"# HistoryRelativePath \r\n",
					"# SourceColumnNames \r\n",
					"# SourceKeysWithNotNullCheck "
				],
				"execution_count": 37
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"SourceTableName --> FolderName\r\n",
					"DF_SourceRaw --> DF_Raw\r\n",
					"registerTempTable --> createOrReplaceTempView\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# **Read Raw Data**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Reading Raw File \r\n",
					"RawADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s/%s' % (RawContainerName, RawStorageAccountName,SourceSystemName, RawRelativePath, FolderName)\r\n",
					"print(RawADLSPath)\r\n",
					"\r\n",
					"#print (RawADLSPath +  \"/*/*/*/\"+ Raw_File_Name)\r\n",
					"\r\n",
					"# #Read with basePath option (Partition Discovery)\r\n",
					"DF_Raw = spark.read.option(\"basePath\", RawADLSPath).parquet(RawADLSPath + \"/*/*/*/\"+ Raw_File_Name)\r\n",
					"\r\n",
					"# display(DF_Raw)\r\n",
					""
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Row Count Check"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# SourceRowCount = DF_Raw.count()\r\n",
					"# # If you want to exit a value from the notebook, use the below command\r\n",
					"# mssparkutils.notebook.exit(SourceRowCount) \r\n",
					"\r\n",
					"DF_Raw.cache()\r\n",
					"SourceRowCount = DF_Raw.count()\r\n",
					"print (SourceRowCount)\r\n",
					"\r\n",
					"#Generate Temp Table for raw file\r\n",
					"DF_Raw.createOrReplaceTempView(f\"TempTable_{FolderName}\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"where_condition = \"is not null\"  # Replace this with your actual WHERE condition\r\n",
					"print(SourceKeysWithNotNullCheck)\r\n",
					"\r\n",
					"# Convert the column names to a list\r\n",
					"\r\n",
					"columns = [col.strip() for col in SourceKeysWithNotNullCheck.split(',')]\r\n",
					"print(columns)\r\n",
					"\r\n",
					"# Construct the SELECT clause dynamically\r\n",
					"\r\n",
					"select_clause = \", \".join(f\"`{col}`\" for col in columns)\r\n",
					"print(select_clause)\r\n",
					"\r\n",
					"\r\n",
					"where_clause = \" AND \".join(f\"`{col}` IS NOT NULL\" for col in columns)\r\n",
					"\r\n",
					"if where_condition:\r\n",
					"\r\n",
					"    where_clause = f\"({where_clause})\"\r\n",
					"\r\n",
					"print(where_clause)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Valid Records"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# DF_ValidRecords = spark.sql(f\"\"\"SELECT {All_Source_Column_Names} from TempTable_{FolderName} \r\n",
					"#                                   WHERE {where_clause}\"\"\")\r\n",
					"# display(DF_ValidRecords)                                  "
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"DF_ValidRecords = spark.sql(f\"\"\"SELECT * \r\n",
					"                                    FROM (SELECT {All_Source_Column_Names},COUNT(1) as cnt\r\n",
					"                                        FROM TempTable_{FolderName} AD GROUP BY {All_Source_Column_Names} HAVING cnt >=1) t1\r\n",
					"                                    WHERE {where_clause}\"\"\")\r\n",
					"display(DF_ValidRecords)\r\n",
					"#NEEDS TO BE RE VISITED THE LOGIC FOR VALID RECORDS"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"DF_ValidRecords = DF_ValidRecords.drop(\"cnt\")\r\n",
					"DF_Raw = DF_ValidRecords.withColumn(\"LND_File_Timestamp\",lit(LND_File_Timestamp))\r\n",
					"display(DF_Raw)\r\n",
					""
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Handling duplicates**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"\r\n",
					"\r\n",
					"F.row_number().over(\r\n",
					"    Window.partitionBy(\"driver\").orderBy(col(\"unit_count\").desc())\r\n",
					")\r\n",
					"window = Window.partitionBy(\"RECID\").orderBy(col(\"MODIFIEDDATETIME\").desc())\r\n",
					"\r\n",
					"df_win = DF_Raw.withColumn(\"Rank\", dense_rank().over(window))\r\n",
					"\r\n",
					"\r\n",
					"# Add Duplicate_Flag column\r\n",
					"df_win = df_win.withColumn(\"Duplicate_Flag\", when(col(\"Rank\") > 1, 1).otherwise(0))\r\n",
					"\r\n",
					"\r\n",
					"df_Final = df_win.withColumn(\"Create_DTTM\" ,lit(current_timestamp())) \\\r\n",
					"                .withColumn(\"Update_DTTM\" ,lit(current_timestamp()))\r\n",
					"\r\n",
					"display(df_Final)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#writing to clean Folder in Cleanse Layer\r\n",
					"\r\n",
					"#Retrive\r\n",
					"partition = datetime.strptime(LND_File_Timestamp ,\"%Y-%m-%d %H:%M:%S.%f\")\r\n",
					"\r\n",
					"Year = partition.year\r\n",
					"Month = partition.month\r\n",
					"Day= partition.day\r\n",
					"\r\n",
					"\r\n",
					"CleanRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, CleanRelativePath, FolderName)\r\n",
					"print(CleanRecordsADLSPath)\r\n",
					"\r\n",
					"df_Final.write.format(\"delta\")\\\r\n",
					"        .mode(\"overwrite\")\\\r\n",
					"        .saveAsTable(f\"sparkdb.cleanse_{FolderName}\", path = CleanRecordsADLSPath)\r\n",
					"#         .option(\"mergeSchema\", \"false\")\\\r\n",
					"#         # .partitionBy(\"2023\", \"08\", \"22\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"#Display Dataframe Result\r\n",
					"if Debug == True:\r\n",
					"     print(\"Total Records: \" + str(df_Final.count()))\r\n",
					"     display(df_Final.limit(NoOfRecordsToDisplay))\r\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Create Delta table if it does not exist\r\n",
					"# if \r\n",
					"# df_Final.write.format(\"delta\") \\\r\n",
					"#            .mode(\"overwrite\") \\\r\n",
					"#            .saveAsTable(\"sparkdb.History_FolderName\") \r\n",
					"\r\n",
					"# # Now you can append data\r\n",
					"# df_Final.write.format(\"delta\") \\\r\n",
					"#            .mode(\"append\") \\\r\n",
					"#            .saveAsTable(\"sparkdb.History_FolderName\")"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Load History Data**"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# spark.sql(\"DROP TABLE  IF EXISTS sparkdb.history_logisticslocation\")\r\n",
					"# spark.sql(\"DROP TABLE  IF EXISTS sparkdb.history_vendsettlement\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"HistoricalRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, HistoryRelativePath, FolderName)\r\n",
					"print(HistoricalRecordsADLSPath)\r\n",
					"\r\n",
					"#check if the table is present\r\n",
					"table_name = f\"History_{FolderName}\"\r\n",
					"db_name = \"sparkdb\"\r\n",
					"\r\n",
					"tables = spark.catalog.listTables(db_name)\r\n",
					"print(tables)\r\n",
					"\r\n",
					"if any(table.name == table_name for table in tables):\r\n",
					"    print(f\"{table_name} table exists\")\r\n",
					"    df_Final.write.format(\"delta\") \\\r\n",
					"            .mode(\"append\") \\\r\n",
					"            .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
					"else:\r\n",
					"    print(f\"{table_name} does not exist\")\r\n",
					"    df_Final.write.format(\"delta\") \\\r\n",
					"           .mode(\"overwrite\") \\\r\n",
					"           .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath) \r\n",
					"\r\n",
					"\r\n",
					"# try:\r\n",
					"#     spark.read.format(\"delta\").load(f\"delta_db.{table_name}\")\r\n",
					"#     print(f\"{table_name} exists\")\r\n",
					"# except AnalysisException: \r\n",
					"#     print(f\"{table_name} does not exist\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# table_path = f\"delta_db/{table_name}\" \r\n",
					"# path = HistoricalRecordsADLSPath\r\n",
					"\r\n",
					"# if DeltaTable.isDeltaTable(spark, path):\r\n",
					"#     print(f\"{table_name} exists\")\r\n",
					"#     # Now you can append data\r\n",
					"#     df_Final.write.format(\"delta\") \\\r\n",
					"#             .mode(\"append\") \\\r\n",
					"#             .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
					"# else:\r\n",
					"#     print(f\"{table_name} does not exist\")\r\n",
					"#     # Create Delta table if it does not exist\r\n",
					"#     df_Final.write.format(\"delta\") \\\r\n",
					"#            .mode(\"overwrite\") \\\r\n",
					"#            .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath) \r\n",
					"\r\n",
					""
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"# HistoricalRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, HistoryRelativePath, FolderName,LND_File_Name_Pattern)\r\n",
					"# print(HistoricalRecordsADLSPath)\r\n",
					"\r\n",
					"# # Create Delta table if it does not exist\r\n",
					"# df_Final.write.format(\"delta\") \\\r\n",
					"#            .mode(\"overwrite\") \\\r\n",
					"#            .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath) \r\n",
					"\r\n",
					"# df_Final.write.format(\"delta\") \\\r\n",
					"#         .mode(\"append\") \\\r\n",
					"#         .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
					"\r\n",
					"# df_Final.write.format(\"delta\")\\\r\n",
					"#         .mode(\"append\")\\\r\n",
					"#         .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\\\r\n",
					"# #        .option(\"mergeSchema\", \"false\")\\\r\n",
					"# #         # .partitionBy(\"2023\", \"08\", \"22\")"
				],
				"execution_count": 84
			}
		]
	}
}
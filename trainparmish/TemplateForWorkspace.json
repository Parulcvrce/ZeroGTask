{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "trainparmish"
		},
		"AzureBlobStorage1_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage1'"
		},
		"AzureBlobStorage2_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage2'"
		},
		"AzureBlobStorage3_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'AzureBlobStorage3'"
		},
		"AzureDataLakeStorage2_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage2'"
		},
		"LS_ASQL_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'LS_ASQL'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=@{linkedService().ASQL_ServerName};Initial Catalog=@{linkedService().ASQLDB_Name}"
		},
		"trainparmish-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'trainparmish-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:trainparmish.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://rawtrg.dfs.core.windows.net/"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synapsesazerog.dfs.core.windows.net/"
		},
		"raw_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://rawtrg.dfs.core.windows.net/"
		},
		"trainparmish-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synmishtrain.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 3')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"method": "POST",
							"headers": {},
							"url": {
								"value": "https://idp.nordic.confirmit.com/identity/connect/token",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"body": {
								"value": "{\n    \"client_id\": \"67b37746-5c35-4f0e-8e54-c9f6e5ab0abd\",\n    \"client_secret\": \"4b28c286-cf29-4539-83a6-aac5a99195ed\",\n    \"grant_type\": \"api-user\",\n    \"scope\": \"pub.surveys\"\n}",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-04-19T08:37:22Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 4')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Web1",
						"type": "WebActivity",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"method": "POST",
							"headers": {
								"Content-Type": "application/x-www-form-urlencoded"
							},
							"url": {
								"value": "https://idp.nordic.confirmit.com/identity/connect/token",
								"type": "Expression"
							},
							"connectVia": {
								"referenceName": "AutoResolveIntegrationRuntime",
								"type": "IntegrationRuntimeReference"
							},
							"body": {
								"value": "@concat('client_id=', '67b37746-5c35-4f0e-8e54-c9f6e5ab0abd', '&client_secret=', '4b28c286-cf29-4539-83a6-aac5a99195ed', '&grant_type=api-user', '&scope=pub.surveys')",
								"type": "Expression"
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-04-19T08:50:51Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage1_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage2_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage3')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage3_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LS_ASQL')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"ASQL_ServerName": {
						"type": "string"
					},
					"ASQLDB_Name": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "AzureSqlDatabase",
				"typeProperties": {
					"connectionString": "[parameters('LS_ASQL_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/raw')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('raw_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainparmish-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('trainparmish-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/trainparmish-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('trainparmish-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/IRsyn')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 10,
							"cleanup": false,
							"customProperties": []
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/NB_Cleanse_Layer_Incremental_Load')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5e7bd3c6-fccc-491f-9917-9c6a65235ab6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/35c7c256-febc-4c53-82a5-71e8573a3c24/resourceGroups/rg-pmishra-training-001/providers/Microsoft.Synapse/workspaces/trainparmish/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://trainparmish.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# <h1><b><center>Raw To Cleanse Layer</center></b></h1>\r\n",
							"\r\n",
							"## Initialise"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# spark = SparkSession.builder \\\r\n",
							"#     .appName(\"MySparkJob\") \\\r\n",
							"#     .config(\"spark.executor.cores\", \"12\") \\\r\n",
							"#     .getOrCreate()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql import Row\r\n",
							"from pyspark.sql.types import StringType,StructType\r\n",
							"from datetime import *\r\n",
							"from delta.tables import *\r\n",
							"from pyspark.sql import DataFrame\r\n",
							"import pandas as pd\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from delta.tables import *\r\n",
							"from delta.tables import DeltaTable\r\n",
							"import copy\r\n",
							"from pyspark.sql.functions import lit\r\n",
							"\r\n",
							"\r\n",
							"#Debug\r\n",
							"Debug = True\r\n",
							"NoOfRecordsToDisplay = 1000\r\n",
							"\r\n",
							"#Set Spark Config\r\n",
							"spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\r\n",
							"spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\", \"true\")\r\n",
							"spark.conf.set(\"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\", \"true\")\r\n",
							"spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",10485760)\r\n",
							"spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\r\n",
							"spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",\"true\")\r\n",
							"spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism * 2)\r\n",
							"#Config needed to read datetime column values without error [Don't remove]\r\n",
							"spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")\r\n",
							"spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\r\n",
							"spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\r\n",
							"spark.conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# #Parameters\r\n",
							"# RawContainerName = ''\r\n",
							"# RawStorageAccountName = ''\r\n",
							"# SourceSystemName = ''\r\n",
							"# RawRelativePath = ''\r\n",
							"# FolderName = ''\r\n",
							"# RawFileName = ''\r\n",
							"# CleanseContainerName = ''\r\n",
							"# CleanseStorageAccountName = ''\r\n",
							"# CleanRelativePath = ''\r\n",
							"# HistoryRelativePath = ''\r\n",
							"# SourceNonKeyColumnNames = ''\r\n",
							"# SourceBusinessKeys  = ''\r\n",
							""
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # #Parameters\r\n",
							"# RawContainerName = 'rawlayer'\r\n",
							"# RawStorageAccountName = 'azsandboxweudlslake001'\r\n",
							"# SourceSystemName = 'AX'\r\n",
							"# RawRelativePath = '/Inbound/'\r\n",
							"# FolderName = 'LogisticsLocation'\r\n",
							"# CleanseContainerName = 'cleansedlayer'\r\n",
							"# CleanseStorageAccountName = 'azsandboxweudlslake001'\r\n",
							"# CleanRelativePath = '/Clean/'\r\n",
							"# HistoryRelativePath = '/History/'\r\n",
							"# All_Source_Column_Names = 'RECID,CREATEDDATETIME,DESCRIPTION,DUNSNUMBERRECID,ISPOSTALADDRESS,LOCATIONID,MODIFIEDBY,MODIFIEDDATETIME,PARENTLOCATION,PARTITION,RECVERSION'\r\n",
							"# SourceKeysWithNotNullCheck = 'RECID IS NOT NULL'\r\n",
							"# # SourceKeysWithNotNullCheck = 'RECID'\r\n",
							"# SourceSystem = 'SourceSystem'\r\n",
							"# LND_File_Timestamp ='2023-08-22 12:56:54.000'\r\n",
							"# RawFileName = 'LogisticsLocation_Data_2023.34.01_02.34.26_1.parquet'\r\n",
							"# #Need to pass#\r\n",
							"# CleanseContainerName ='cleansedlayer'\r\n",
							"# SourceBusinessKeys = 'RECID'\r\n",
							"\r\n",
							"# # OpenDate\r\n",
							"# # OpenDateValue = \"2023-08-22T02:39:10Z\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"SourceTableName --> FolderName\r\n",
							"DF_SourceRaw --> DF_Raw\r\n",
							"registerTempTable --> createOrReplaceTempView\r\n",
							""
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Read Raw Data"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Reading Raw File \r\n",
							"RawADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s/%s' % (RawContainerName, RawStorageAccountName,SourceSystemName, RawRelativePath, FolderName)\r\n",
							"print(RawADLSPath)\r\n",
							"\r\n",
							"\r\n",
							"# #Read with basePath option (Partition Discovery)\r\n",
							"DF_Raw = spark.read.option(\"basePath\", RawADLSPath).parquet(RawADLSPath + \"/*/*/*/\"+ RawFileName)\r\n",
							"\r\n",
							"# display(DF_Raw)\r\n",
							"DF_Raw.printSchema()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Row Count Check**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# # If you want to exit a value from the notebook, use the below command\r\n",
							"# mssparkutils.notebook.exit(SourceRowCount) \r\n",
							"\r\n",
							"DF_Raw.cache()\r\n",
							"SourceRowCount = DF_Raw.count()\r\n",
							"print (SourceRowCount)\r\n",
							"\r\n",
							"DF_Rawwithlnd = DF_Raw.withColumn(\"LND_File_Timestamp\" ,lit(LND_File_Timestamp))\r\n",
							"# display(DF_Rawwithlnd)\r\n",
							"DF_Rawwithlnd.printSchema()\r\n",
							"\r\n",
							"#Generate Temp Table for raw file\r\n",
							"DF_Rawwithlnd.createOrReplaceTempView(f\"TempRawTable_{FolderName}\")\r\n",
							"new_data_df = spark.table(f\"TempRawTable_{FolderName}\")\r\n",
							"\r\n",
							"# print(f\"TempRawTable_{FolderName}\")\r\n",
							"# spark.sql(\"select * from TempRawTable_LogisticsLocation\").show()\r\n",
							"# new_data_df = spark.table(\"TempRawTable_LogisticsLocation\")\r\n",
							"# display(new_data_df)\r\n",
							"# if Debug == True:\r\n",
							"#     print(\"RawADLSPath:\", RawADLSPath)\r\n",
							"#     print(\"SourceRowCount:\", SourceRowCount)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Read Cleansed Data**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Set Curated filepath \r\n",
							"CleansedADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, CleanRelativePath, FolderName)\r\n",
							"\r\n",
							"#CleanRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, CleanRelativePath, FolderName)\r\n",
							"print(CleansedADLSPath)\r\n",
							"\r\n",
							"\r\n",
							"#Verify if delta table exists\r\n",
							"try:\r\n",
							"    NoDeltaTable = False\r\n",
							"    Result = mssparkutils.fs.ls(CleansedADLSPath + \"/_delta_log\")\r\n",
							"\r\n",
							"except:\r\n",
							"    NoDeltaTable = True     \r\n",
							"\r\n",
							"#Check if Delta Table exists\r\n",
							"if NoDeltaTable == False:\r\n",
							"    #Define the Delta Table\r\n",
							"    DT_Cleansed = DeltaTable.forPath(spark, CleansedADLSPath)\r\n",
							"    #Define the Dataframe\r\n",
							"    DF_Cleansed = spark.read.format(\"delta\").load(CleansedADLSPath)\r\n",
							"    #Register the Temp Table\r\n",
							"    DF_Cleansed.registerTempTable(f\"TempTable_{FolderName}Cleansed\")\r\n",
							"\r\n",
							"#Display Result\r\n",
							"if Debug == True:\r\n",
							"    print(\"NoDeltaTable:\", NoDeltaTable)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# tableName = f\"TempTable_{FolderName}Cleansed\"\r\n",
							"DF_Cleansed.registerTempTable(f\"TempTable_{FolderName}Cleansed\")\r\n",
							"# print(f\"TempTable_{FolderName}Cleansed\")\r\n",
							"\r\n",
							"#### to view the target data before upsert#######\r\n",
							"cleandf = spark.table(f\"TempTable_{FolderName}Cleansed\")\r\n",
							"# cleandf.show()\r\n",
							"\r\n",
							"####register table to view#####\r\n",
							"cleandf.createOrReplaceTempView(f\"TempcleansedTable_{FolderName}\")\r\n",
							"spark.sql(f\"select * from TempcleansedTable_{FolderName}\")\r\n",
							"\r\n",
							"# spark.sql(\"select * from TempTable_LogisticsLocationCleansed\")\r\n",
							"\r\n",
							"###snapshot before upsert####\r\n",
							"original_snaptrg = spark.table(f\"TempTable_{FolderName}Cleansed\").alias(\"orig\")\r\n",
							"original_snaptrg.show()\r\n",
							"original_snaptrg.count()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Merge Operation**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"##list of src row col for merge####\r\n",
							"columns_to_update = All_Source_Column_Names.split(',') \r\n",
							"# columns_to_update.append('LND_File_Timestamp')\r\n",
							"\r\n",
							"# columns_to_update.append('LND_File_Timestamp')\r\n",
							"Incol = copy.copy(columns_to_update)\r\n",
							"Incol.append('LND_File_Timestamp')\r\n",
							"\r\n",
							"####Key col value for merge##########\r\n",
							"Joinkey_col = SourceBusinessKeys.split(',') \r\n",
							"# merge_condition = \"DF_Cleansed.RECID = DF_Raw.RECID\"\r\n",
							"\r\n",
							"######Merge condition ##########################\r\n",
							"merge_conds = []\r\n",
							"\r\n",
							"for Key in Joinkey_col:\r\n",
							"    cond = f\"TempcleansedTable_{FolderName}.{Key} = TempRawTable_{FolderName}.{Key}\"\r\n",
							"    # cond = f\"TempcleansedTable_LogisticsLocation.{Key} = TempRawTable_LogisticsLocation.{Key}\"\r\n",
							"    merge_conds.append(cond)\r\n",
							"\r\n",
							"merge_condition = \" AND \".join(merge_conds)\r\n",
							"\r\n",
							"insert_cols = \", \".join(Incol)\r\n",
							"insert_vals = \", \".join([f\"TempRawTable_{FolderName}.{col}\" for col in Incol])\r\n",
							"# print(insert_vals)\r\n",
							"\r\n",
							"\r\n",
							"set_clauses = []\r\n",
							"for col in Incol:\r\n",
							"  # set_clause = \"TempcleansedTable_LogisticsLocation.{0} = TempRawTable_LogisticsLocation.{0}\".format(col)\r\n",
							"  set_clause = f\"TempcleansedTable_{FolderName}.{col} = TempRawTable_{FolderName}.{col}\".format(col)\r\n",
							"  set_clauses.append(set_clause)\r\n",
							"set_stmt = \", \".join(set_clauses)\r\n",
							"# print(set_stmt)\r\n",
							"\r\n",
							"\r\n",
							"tablecleansed = f\"TempcleansedTable_{FolderName}\"\r\n",
							"tableraw = f\"TempRawTable_{FolderName}\"\r\n",
							"\r\n",
							"\r\n",
							"#############Merge Operation##################\r\n",
							"\r\n",
							"merge_stmt = \"\"\"\r\n",
							"  MERGE INTO {tablecleansed} USING {tableraw}\r\n",
							"  ON {merge_cond}\r\n",
							"  WHEN MATCHED THEN \r\n",
							"    UPDATE SET \r\n",
							"      {set_stmt} /*rows updated*/\r\n",
							"  WHEN NOT MATCHED THEN \r\n",
							"    INSERT ({insert_cols}) VALUES ({insert_vals}) /* rows inserted */\r\n",
							"\"\"\".format(merge_cond=merge_condition,\r\n",
							"set_stmt = set_stmt,\r\n",
							"insert_cols = insert_cols,\r\n",
							"insert_vals = insert_vals,\r\n",
							"tablecleansed= tablecleansed,\r\n",
							"tableraw= tableraw\r\n",
							")\r\n",
							"\r\n",
							"merged_trg = spark.sql(merge_stmt)\r\n",
							"\r\n",
							"\r\n",
							"# ####Spare testing code#########\r\n",
							"# spark.table(\"select * from TempFinalTable_LogisticsLocation\").show()\r\n",
							"# merged_trg.createOrReplaceTempView(f\"TempFinalTable_{FolderName}\")\r\n",
							"# spark.sql(\"select * from TempFinalTable_LogisticsLocation\").show(\r\n",
							"# spark.sql(\"select * from TempFinalTable_LogisticsLocation\").show()\r\n",
							"# display(TempFinalTable_LogisticsLocation)\r\n",
							"# DfUpdatedCleansedTable = spark.table(DF_Cleansed)\r\n",
							"# final = DF_Cleansed.union(merged_df)\r\n",
							"# DF_Cleansed.show()\r\n",
							"# spark.sql(f\"Select * from TempRawTable_{FolderName}\"). show()\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#####Spare testing code#########\r\n",
							"# spark.table(\"select * from TempFinalTable_LogisticsLocation\").show()\r\n",
							"# merged_trg.createOrReplaceTempView(f\"TempFinalTable_{FolderName}\")\r\n",
							"# spark.sql(f\"select * from TempFinalTable_{FolderName}\").show()\r\n",
							"# # spark.sql(\"select * from TempFinalTable_LogisticsLocation\").show()\r\n",
							"# display(TempFinalTable_LogisticsLocation)\r\n",
							"# DfUpdatedCleansedTable = spark.table(DF_Cleansed)\r\n",
							"# final = DF_Cleansed.union(merged_df)\r\n",
							"# DF_Cleansed.show()\r\n",
							"# spark.sql(f\"Select * from TempRawTable_{FolderName}\"). show()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# # writing to clean Folder in Cleanse Layer\r\n",
							"# CleanRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, CleanRelativePath, FolderName)\r\n",
							"# DF_Raw.write.format(\"delta\")\\\r\n",
							"#         .mode(\"overwrite\")\\\r\n",
							"#         .saveAsTable(f\"sparkdb.cleanse_{FolderName}\", path = CleanRecordsADLSPath)\r\n",
							"# #        .option(\"mergeSchema\", \"false\")\\\r\n",
							"# #       .partitionBy(\"2023\", \"08\", \"10\")\\"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Load History Record**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# HistoricalRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, HistoryRelativePath, FolderName,LND_File_Name_Pattern)\r\n",
							"# print(HistoricalRecordsADLSPath)\r\n",
							"# DF_Rawwithlnd.write.format(\"delta\")\\\r\n",
							"#         .mode(\"append\")\\\r\n",
							"#         .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
							"# #         .option(\"mergeSchema\", \"false\")\\\r\n",
							"# #         # .partitionBy(\"2023\", \"08\", \"22\")\r\n",
							"\r\n",
							"LND_File_Name_Pattern = 'LogisticsLocation_data'\r\n",
							"HistoricalRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, HistoryRelativePath, FolderName)\r\n",
							"print(HistoricalRecordsADLSPath)\r\n",
							"\r\n",
							"#check if the table is present\r\n",
							"table_name = f\"History_{FolderName}\"\r\n",
							"db_name = \"sparkdb\"\r\n",
							"\r\n",
							"tables = spark.catalog.listTables(db_name)\r\n",
							"print(tables)\r\n",
							"\r\n",
							"if any(table.name == table_name for table in tables):\r\n",
							"    print(f\"{table_name} table exists\")\r\n",
							"    DF_Rawwithlnd.write.format(\"delta\") \\\r\n",
							"            .mode(\"append\") \\\r\n",
							"            .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
							"else:\r\n",
							"    print(f\"{table_name} does not exist\")\r\n",
							"    DF_Rawwithlnd.write.format(\"delta\") \\\r\n",
							"           .mode(\"overwrite\") \\\r\n",
							"           .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"HistoricalRecordsADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s%s%s/%s/' % (CleanseContainerName, CleanseStorageAccountName, SourceSystemName, HistoryRelativePath, FolderName,LND_File_Name_Pattern)\r\n",
							"print(HistoricalRecordsADLSPath)\r\n",
							"DF_Rawwithlnd.write.format(\"delta\")\\\r\n",
							"        .mode(\"append\")\\\r\n",
							"        .saveAsTable(f\"sparkdb.History_{FolderName}\", path = HistoricalRecordsADLSPath)\r\n",
							"#         .option(\"mergeSchema\", \"false\")\\\r\n",
							"#         # .partitionBy(\"2023\", \"08\", \"22\")\r\n",
							"\r\n",
							"DF_test = spark.sql(saveAsTable.withcolumn(\"colum\"))"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "80d6c994-7331-4ba5-81fa-1ebf43b2981d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/35c7c256-febc-4c53-82a5-71e8573a3c24/resourceGroups/rg-pmishra-training-001/providers/Microsoft.Synapse/workspaces/trainparmish/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://trainparmish.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\n",
							"import pandas as pd\n",
							"blob_account_name = \"strgpmishra\"\n",
							"blob_container_name = \"raw\"\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"sc = SparkSession.builder.getOrCreate()\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"blob_sas_token = token_library.getConnectionString(\"AzureBlobStorage1\")\n",
							"\n",
							"spark.conf.set(\n",
							"    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
							"    blob_sas_token)\n",
							"df = spark.read.load('wasbs://raw@strgpmishra.blob.core.windows.net/Country_Rollup.csv', format='csv'\n",
							"## If header exists uncomment line below\n",
							", header=True\n",
							")\n",
							"df = df.na.fill(\"\")\n",
							"# Or using fillna\n",
							"df = df.fillna(\"\")\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.functions import to_date, date_add\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"from datetime import datetime, timedelta\r\n",
							"\r\n",
							"# Initialize SparkSession (assuming you have already done this; if not, uncomment the next two lines)\r\n",
							"# spark = SparkSession.builder \\\r\n",
							"#     .appName(\"Date Issue Fix\") \\\r\n",
							"#     .getOrCreate()\r\n",
							"\r\n",
							"# Corrected sample data as a list of tuples\r\n",
							"data = [(\"2023-03-31\",), (\"2022-03-31\",)]  \r\n",
							"\r\n",
							"# Define schema\r\n",
							"schema = StructType([ \r\n",
							"    StructField(\"FiscalPeriodEndDate\", StringType(), True)\r\n",
							"])\r\n",
							"\r\n",
							"# Create DataFrame with the correct structure\r\n",
							"df = spark.createDataFrame(data, schema)\r\n",
							"df.createOrReplaceTempView(\"TempTable_FiscalPeriod\")\r\n",
							"\r\n",
							"df = spark.sql(\"SELECT * FROM TempTable_FiscalPeriod\")\r\n",
							"df.show()\r\n",
							"\r\n",
							"# Using date_add for date arithmetic\r\n",
							"DF_StartEndDates = spark.sql(\"\"\"\r\n",
							"    SELECT \r\n",
							"      date_add(CAST(MIN(FiscalPeriodEndDate) AS DATE), 1) AS FiscalStartDate,  \r\n",
							"      date_add(CAST(MAX(FiscalPeriodEndDate) AS DATE), 1) AS FiscalEndDate\r\n",
							"    FROM TempTable_FiscalPeriod\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"DF_StartEndDates.show()\r\n",
							"\r\n",
							"#Create List from DF_StartEndDates\r\n",
							"DatesCollection = DF_StartEndDates.collect()\r\n",
							"\r\n",
							"#Generate Date Ranges from Start & End Dates \r\n",
							"def DimDate_PeriodInDaysDateRange(start_date, end_date):\r\n",
							"    for n in range(int((end_date - start_date).days)):\r\n",
							"        yield start_date + timedelta(n)\r\n",
							"\r\n",
							"#Initialize list to store dates between Start & End Date\r\n",
							"List_DimDatePeriodInDaysDates = []\r\n",
							"\r\n",
							"#Loop in DatesCollection and generate all days between Start & End Date\r\n",
							"for row in DatesCollection:\r\n",
							"    PeriodInDaysStartDate = row[\"FiscalStartDate\"]\r\n",
							"    PeriodInDaysEndDate =  row[\"FiscalEndDate\"]\r\n",
							"    for single_date in DimDate_PeriodInDaysDateRange(PeriodInDaysStartDate, PeriodInDaysEndDate):\r\n",
							"        #Append single days to Period In Days Dates List\r\n",
							"        List_DimDatePeriodInDaysDates.append(single_date.strftime(\"%Y-%m-%d\"))\r\n",
							"\r\n",
							"#Define the Dataframe from Period In Days Dates List\r\n",
							"DF_PeriodInDaysDates = spark.createDataFrame(List_DimDatePeriodInDaysDates, StringType())\r\n",
							"DF_PeriodInDaysDates.createOrReplaceTempView('FiscalDates')\r\n",
							"\r\n",
							"\r\n",
							"# Define HistoricalCalendar as \"Gregorian\" for the United Kingdom calendar\r\n",
							"HistoricalCalendar = \"Gregorian\"\r\n",
							"\r\n",
							"GenDates = spark.sql(f\"\"\"\r\n",
							"    SELECT \r\n",
							"        date_add(CAST(MIN(FiscalPeriodEndDate) AS DATE), 1) AS FiscalPeriodStartDate,\r\n",
							"        date_add(CAST(MAX(FiscalPeriodEndDate) AS DATE), 1) AS FiscalPeriodEndDate,\r\n",
							"        '{HistoricalCalendar}' AS Calendar,\r\n",
							"        YEAR(FiscalPeriodEndDate) AS FiscalYear,\r\n",
							"        CONCAT('Period ', month(FiscalPeriodEndDate)) AS PeriodName,\r\n",
							"        MONTH(FiscalPeriodEndDate) AS FiscalMonth,\r\n",
							"        CASE \r\n",
							"            WHEN MONTH(FiscalPeriodEndDate) BETWEEN 1 AND 3 THEN 1\r\n",
							"            WHEN MONTH(FiscalPeriodEndDate) BETWEEN 4 AND 6 THEN 2\r\n",
							"            WHEN MONTH(FiscalPeriodEndDate) BETWEEN 7 AND 9 THEN 3\r\n",
							"            ELSE 4\r\n",
							"        END AS FiscalQuarter,\r\n",
							"        1 AS Type,\r\n",
							"        current_timestamp() AS SyncStartDateTime\r\n",
							"    FROM TempTable_FiscalPeriod\r\n",
							"    GROUP BY FiscalYear, PeriodName, FiscalMonth, FiscalQuarter\r\n",
							"\"\"\")\r\n",
							"# Register the Temp Table\r\n",
							"GenDates.createOrReplaceTempView(\"TempTable_FiscalPeriodAdditionalDates\")\r\n",
							"\r\n",
							"# df_dates = spark.sql(\"SELECT * FROM TempTable_FiscalPeriodAdditionalDates\")\r\n",
							"# df_dates.show()\r\n",
							"\r\n",
							"# Define the Dataframe\r\n",
							"DF_Calendar = spark.sql(f\"\"\"SELECT  \r\n",
							"                                '-1' AS DateCode,\r\n",
							"                                to_date('1900-01-01') AS Date,\r\n",
							"                                to_date('1989-01-01') AS PreviousYearDate,\r\n",
							"                                '190001' AS CalendarPeriod,\r\n",
							"                                'Monday, January 1 1900' AS DateName,\r\n",
							"                                '1900' AS CalendarYear,\r\n",
							"                                'Year 1900' AS CalendarYearName,\r\n",
							"                                '1900-Q1' AS CalendarQuarter,\r\n",
							"                                'Quarter 1, 1900' AS CalendarQuarterName,\r\n",
							"                                '01' AS CalendarMonth,\r\n",
							"                                'January' AS MonthShortName,\r\n",
							"                                'January, 1900' AS CalendarMonthName,\r\n",
							"                                '01' AS CalendarWeek,\r\n",
							"                                'Week 01, 1900' AS CalendarWeekName,\r\n",
							"                                '001' AS CalendarDayOfYear,\r\n",
							"                                'Day 001, 1900' AS CalendarDayOfYearName,\r\n",
							"                                '01' AS DayOfMonth,\r\n",
							"                                'Day 01' AS DayOfMonthName,\r\n",
							"                                '1' AS DayOfWeek,\r\n",
							"                                'Day 1' AS DayOfWeekName,\r\n",
							"                                'Monday' AS Weekday,\r\n",
							"                                '1900, 01' AS CalendarWeekOfYear,\r\n",
							"                                'Week 01, 1900' AS CalendarWeekOfYearName,\r\n",
							"                                '1900, 01' AS CalendarMonthOfYear,\r\n",
							"                                'Month 01, 1900' AS CalendarMonthOfYearName,\r\n",
							"                                '1900, 1' AS CalendarQuarterOfYear,\r\n",
							"                                'Quarter 1, 1900' AS CalendarQuarterOfYearName,\r\n",
							"                                '1900-H1' AS CalendarHalfYear,\r\n",
							"                                '190001' AS FiscalPeriod,\r\n",
							"                                '1900' AS FiscalYear,\r\n",
							"                                'Fiscal Year 1900' AS FiscalYearName,\r\n",
							"                                '1900-Q1' AS FiscalQuarter,\r\n",
							"                                'Fiscal Quarter 1, 1900' AS FiscalQuarterName,\r\n",
							"                                '01' AS FiscalMonth,\r\n",
							"                                'January, 1900' AS FiscalMonthName,\r\n",
							"                                '1900-H1' AS FiscalHalfYear,\r\n",
							"                                '' AS Calendar\r\n",
							"                        UNION ALL\r\n",
							"                        SELECT  Calendar.DateCode,\r\n",
							"                                to_date(Calendar.Date) AS Date,\r\n",
							"                                ADD_MONTHS(to_date(Calendar.Date),-12) AS PreviousYearDate,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'),date_format(Calendar.Date, 'MM')) AS CalendarPeriod,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'EEEE'), \", \" ,date_format(Calendar.Date, 'MMMM'), \" \"  , date_format(Calendar.Date, 'd'),  \" \" , date_format(Calendar.Date, 'y')) AS DateName,\r\n",
							"                                date_format(Calendar.Date, 'y') AS CalendarYear,\r\n",
							"                                CONCAT(\"Year \", date_format(Calendar.Date, 'y')) AS CalendarYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), '-Q', date_format(Calendar.Date, 'q')) AS CalendarQuarter,\r\n",
							"                                CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterName,\r\n",
							"                                RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2) AS CalendarMonth,\r\n",
							"                                date_format(Calendar.Date, 'MMMM') AS MonthShortName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'MMMM'), \", \", date_format(Calendar.Date, 'y') ) AS CalendarMonthName,\r\n",
							"                                RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2) AS CalendarWeek,\r\n",
							"                                CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \",date_format(Calendar.Date, 'y')) AS CalendarWeekName,\r\n",
							"                                RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3) AS CalendarDayOfYear,\r\n",
							"                                CONCAT(\"Day \", RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3), \", \",date_format(Calendar.Date, 'y')) AS CalendarDayOfYearName,\r\n",
							"                                RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2) AS DayOfMonth,\r\n",
							"                                CONCAT(\"Day \", RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2)) AS DayOfMonthName,\r\n",
							"                                dayofweek(Calendar.Date) AS DayOfWeek,\r\n",
							"                                CONCAT(\"Day \",dayofweek(Calendar.Date)) AS DayOfWeekName,\r\n",
							"                                date_format(Calendar.Date, 'EEEE') AS Weekday,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'),', ',RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2)) AS CalendarWeekOfYear,\r\n",
							"                                CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \" ,date_format(Calendar.Date, 'y')) AS CalendarWeekOfYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), \", \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2)) AS CalendarMonthOfYear,\r\n",
							"                                CONCAT(\"Month \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2), \", \",date_format(Calendar.Date, 'y') ) AS CalendarMonthOfYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), \", \", date_format(Calendar.Date, 'q')) AS CalendarQuarterOfYear,\r\n",
							"                                CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterOfYearName,\r\n",
							"                                CASE \r\n",
							"                                    WHEN int(date_format(Calendar.Date, 'q')) < 3\r\n",
							"                                    THEN CONCAT(date_format(Calendar.Date, 'y'), '-H1')\r\n",
							"                                    ELSE CONCAT(date_format(Calendar.Date, 'y'), '-H2')\r\n",
							"                                END AS CalendarHalfYear,\r\n",
							"                                CONCAT(RIGHT(Calendar.FiscalYear,4), right(CONCAT('0',FPO.FiscalMonth),2)) AS FiscalPeriod,\r\n",
							"                                RIGHT(Calendar.FiscalYear,4) AS FiscalYear,\r\n",
							"                                CONCAT(\"Fiscal Year \", RIGHT(Calendar.FiscalYear,4)) AS FiscalYearName,\r\n",
							"                                CONCAT(RIGHT(Calendar.FiscalYear,4), '-Q', FPO.FiscalQuarter ) AS FiscalQuarter,\r\n",
							"                                CONCAT(\"Fiscal Quarter \", FPO.FiscalQuarter, \", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalQuarterName,\r\n",
							"                                RIGHT(CONCAT('0', FPO.FiscalMonth),2) AS FiscalMonth,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'MMMM'),\", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalMonthName,\r\n",
							"                                CASE \r\n",
							"                                    WHEN FPO.FiscalQuarter < 3\r\n",
							"                                    THEN CONCAT(RIGHT(Calendar.FiscalYear,4), '-H1')\r\n",
							"                                    ELSE CONCAT(RIGHT(Calendar.FiscalYear,4), '-H2')\r\n",
							"                                END AS FiscalHalfYear,\r\n",
							"                                Calendar.Calendar AS Calendar\r\n",
							"                        FROM \r\n",
							"                            (SELECT CONCAT(FPI.CALENDAR, '_' ,FD.value) AS DateCode,\r\n",
							"                                    FPI.CALENDAR,\r\n",
							"                                    FPI.FISCALYEAR,\r\n",
							"                                    FD.value AS Date\r\n",
							"                             FROM TempTable_FiscalPeriodAdditionalDates FPI\r\n",
							"                             CROSS JOIN FiscalDates FD\r\n",
							"                             WHERE FD.Value BETWEEN FPI.FiscalPeriodStartDate AND FPI.FiscalPeriodEndDate) AS Calendar\r\n",
							"                             LEFT JOIN TempTable_FiscalPeriodAdditionalDates FPO\r\n",
							"                                  ON Calendar.Date BETWEEN FPO.FiscalPeriodStartDate AND FPO.FiscalPeriodEndDate\r\n",
							"                                  AND Calendar.Calendar = FPO.Calendar\r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"DF_Calendar.createOrReplaceTempView(\"TempTable_FiscalPeriodCalender\")\r\n",
							"df_calender = spark.sql(\"SELECT * FROM TempTable_FiscalPeriodCalender\")\r\n",
							"# df_calender.show()\r\n",
							"display(df_calender)\r\n",
							"\r\n",
							"# #Write cleansed data\r\n",
							"# DF_Calendar.write.format(\"delta\")\\\r\n",
							"#                  .mode(\"overwrite\")\\\r\n",
							"#                  .option(\"overwriteSchema\", \"false\")\\\r\n",
							"#                  .save(CuratedADLSPath)\r\n",
							"\r\n",
							"# #Display Dataframe Result\r\n",
							"# if Debug == True:\r\n",
							"#     print(\"Total Records: \" + str(DF_Calendar.count()))\r\n",
							"#     display(DF_Calendar.take(NoOfRecordsToDisplay))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql.functions import to_date\r\n",
							"from datetime import date\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType\r\n",
							"\r\n",
							"# Hardcoded start and end dates\r\n",
							"GeneratedDatesStartDate = \"2023-04-01\"  # Example: April 1st, 2023\r\n",
							"HistoricalCalendar = \"Gregorian\"  # Example: Gregorian calendar\r\n",
							"\r\n",
							"# # Curated Target Folder Path\r\n",
							"CuratedADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s' % ('trainsyn', 'synmishtrain', 'test' +\"/\" + \"Dates\")\r\n",
							"\r\n",
							"# DF_StartEndDates = spark.sql(f\"\"\"SELECT CAST(\"{GeneratedDatesStartDate}\" AS Date) FiscalStartDate, CAST(MAX(FiscalPeriodEndDate) AS Date)+1 AS FiscalEndDate\r\n",
							"#                                 FROM TempTable_FiscalPeriod\"\"\")\r\n",
							"\r\n",
							"# Sample data\r\n",
							"date1 = str(date(2023, 3, 31)) \r\n",
							"date2 = str(date(2022, 3, 31))\r\n",
							"\r\n",
							"data = [(date1), (date2)]\r\n",
							"\r\n",
							"# Specify schema with string type\r\n",
							"schema = StructType([\r\n",
							"    StructField(\"FiscalPeriodEndDate\", StringType())\r\n",
							"])\r\n",
							"\r\n",
							"# Create DataFrame \r\n",
							"df = spark.createDataFrame(data, schema=schema)\r\n",
							"\r\n",
							"df.createOrReplaceTempView(\"TempTable_FiscalPeriod\") \r\n",
							"\r\n",
							"DF_StartEndDates = spark.sql(\"\"\"\r\n",
							"    SELECT \r\n",
							"      CAST(MIN(FiscalPeriodEndDate) AS DATE) + INTERVAL 1 DAY AS FiscalStartDate,  \r\n",
							"      CAST(MAX(FiscalPeriodEndDate) AS DATE) + INTERVAL 1 DAY AS FiscalEndDate\r\n",
							"    FROM TempTable_FiscalPeriod\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# # Create List from DF_StartEndDates\r\n",
							"# DatesCollection = DF_StartEndDates.collect()\r\n",
							"\r\n",
							"# # Generate Date Ranges from Start & End Dates \r\n",
							"# def DimDate_PeriodInDaysDateRange(start_date, end_date):\r\n",
							"#     for n in range(int((end_date - start_date).days)):\r\n",
							"#         yield start_date + timedelta(n)\r\n",
							"\r\n",
							"# # Initialize list to store dates between Start & End Date\r\n",
							"# List_DimDatePeriodInDaysDates = []\r\n",
							"\r\n",
							"# # Loop in DatesCollection and generate all days between Start & End Date\r\n",
							"# for row in DatesCollection:\r\n",
							"#     PeriodInDaysStartDate = row[\"FiscalStartDate\"]\r\n",
							"#     PeriodInDaysEndDate =  row[\"FiscalEndDate\"]\r\n",
							"#     for single_date in DimDate_PeriodInDaysDateRange(PeriodInDaysStartDate, PeriodInDaysEndDate):\r\n",
							"#         # Append single days to Period In Days Dates List\r\n",
							"#         List_DimDatePeriodInDaysDates.append(single_date.strftime(\"%Y-%m-%d\"))\r\n",
							"\r\n",
							"# # Define the Dataframe from Period In Days Dates List\r\n",
							"# DF_PeriodInDaysDates = spark.createDataFrame(List_DimDatePeriodInDaysDates, StringType())\r\n",
							"# DF_PeriodInDaysDates.createOrReplaceTempView('FiscalDates')\r\n",
							"\r\n",
							"# # Define the Dataframe\r\n",
							"# GenDates = spark.sql(f\"\"\"SELECT FiscalPeriodStartDate,\r\n",
							"#                                FiscalPeriodEndDate,\r\n",
							"#                                Calendar,\r\n",
							"#                                FiscalYear,\r\n",
							"#                                PeriodName,\r\n",
							"#                                FiscalMonth,\r\n",
							"#                                FiscalQuarter,\r\n",
							"#                                Type, \r\n",
							"#                                SyncStartDateTime\r\n",
							"#                         FROM TempTable_FiscalPeriod\r\n",
							"#                         UNION ALL\r\n",
							"#                         (SELECT distinct(date_add(last_day(add_months(Date(FD.value), -1 )),1)) FiscalPeriodStartDate,\r\n",
							"#                                LAST_DAY(FD.value) FiscalPeriodEndDate,\r\n",
							"#                                '{HistoricalCalendar}' AS Calendar,\r\n",
							"#                                CASE \r\n",
							"#                                     WHEN month(FD.value) BETWEEN 7 AND 12 \r\n",
							"#                                         THEN CONCAT('FY', year(FD.value) +1)\r\n",
							"#                                     ELSE CONCAT('FY', year(FD.value)) \r\n",
							"#                                END AS FiscalYear,\r\n",
							"\r\n",
							"#                                CASE \r\n",
							"#                                     WHEN month(FD.value) BETWEEN 7 AND 12\r\n",
							"#                                         THEN CONCAT('Periode ', month(FD.value) - 6)\r\n",
							"#                                     ELSE CONCAT('Periode ', month(FD.value) + 6)\r\n",
							"#                                END AS PeriodName,\r\n",
							"\r\n",
							"#                                CASE \r\n",
							"#                                     WHEN month(FD.value) BETWEEN 7 AND 12\r\n",
							"#                                         THEN month(FD.value) - 6\r\n",
							"#                                     ELSE month(FD.value) + 6\r\n",
							"#                                END AS FiscalMonth,\r\n",
							"#                                CASE \r\n",
							"#                                     WHEN month(FD.value) BETWEEN 7 AND 9 \r\n",
							"#                                         THEN 1\r\n",
							"#                                     WHEN month(FD.value) BETWEEN 10 AND 12\r\n",
							"#                                         THEN 2\r\n",
							"#                                     WHEN month(FD.value) BETWEEN 1 AND 3\r\n",
							"#                                         THEN 3\r\n",
							"#                                     WHEN month(FD.value) BETWEEN 4 AND 6\r\n",
							"#                                         THEN 4\r\n",
							"#                                END AS FiscalQuarter,\r\n",
							"#                                1 AS Type,\r\n",
							"#                                current_timestamp() as SyncStartDateTime                               \r\n",
							"#                                FROM FiscalDates AS FD\r\n",
							"#                                WHERE FD.value < (Select MIN(FiscalPeriodStartDate) from TempTable_FiscalPeriod WHERE Calendar = '{HistoricalCalendar}'))\r\n",
							"#                                \"\"\")\r\n",
							"\r\n",
							"# # Register the Temp Table\r\n",
							"# GenDates.createOrReplaceTempView(\"TempTable_FiscalPeriodAdditionalDates\")\r\n",
							"\r\n",
							"# # Define the Dataframe\r\n",
							"# DF_Calendar = spark.sql(f\"\"\"SELECT  \r\n",
							"#                                 '-1' AS DateCode,\r\n",
							"#                                 to_date('1900-01-01') AS Date,\r\n",
							"#                                 to_date('1989-01-01') AS PreviousYearDate,\r\n",
							"#                                 '190001' AS CalendarPeriod,\r\n",
							"#                                 'Monday, January 1 1900' AS DateName,\r\n",
							"#                                 '1900' AS CalendarYear,\r\n",
							"#                                 'Year 1900' AS CalendarYearName,\r\n",
							"#                                 '1900-Q1' AS CalendarQuarter,\r\n",
							"#                                 'Quarter 1, 1900' AS CalendarQuarterName,\r\n",
							"#                                 '01' AS CalendarMonth,\r\n",
							"#                                 'January' AS MonthShortName,\r\n",
							"#                                 'January, 1900' AS CalendarMonthName,\r\n",
							"#                                 '01' AS CalendarWeek,\r\n",
							"#                                 'Week 01, 1900' AS CalendarWeekName,\r\n",
							"#                                 '001' AS CalendarDayOfYear,\r\n",
							"#                                 'Day 001, 1900' AS CalendarDayOfYearName,\r\n",
							"#                                 '01' AS DayOfMonth,\r\n",
							"#                                 'Day 01' AS DayOfMonthName,\r\n",
							"#                                 '1' AS DayOfWeek,\r\n",
							"#                                 'Day 1' AS DayOfWeekName,\r\n",
							"#                                 'Monday' AS Weekday,\r\n",
							"#                                 '1900, 01' AS CalendarWeekOfYear,\r\n",
							"#                                 'Week 01, 1900' AS CalendarWeekOfYearName,\r\n",
							"#                                 '1900, 01' AS CalendarMonthOfYear,\r\n",
							"#                                 'Month 01, 1900' AS CalendarMonthOfYearName,\r\n",
							"#                                 '1900, 1' AS CalendarQuarterOfYear,\r\n",
							"#                                 'Quarter 1, 1900' AS CalendarQuarterOfYearName,\r\n",
							"#                                 '1900-H1' AS CalendarHalfYear,\r\n",
							"#                                 '190001' AS FiscalPeriod,\r\n",
							"#                                 '1900' AS FiscalYear,\r\n",
							"#                                 'Fiscal Year 1900' AS FiscalYearName,\r\n",
							"#                                 '1900-Q1' AS FiscalQuarter,\r\n",
							"#                                 'Fiscal Quarter 1, 1900' AS FiscalQuarterName,\r\n",
							"#                                 '01' AS FiscalMonth,\r\n",
							"#                                 'January, 1900' AS FiscalMonthName,\r\n",
							"#                                 '1900-H1' AS FiscalHalfYear,\r\n",
							"#                                 '' AS Calendar\r\n",
							"#                         UNION ALL\r\n",
							"#                         SELECT  Calendar.DateCode,\r\n",
							"#                                 to_date(Calendar.Date) AS Date,\r\n",
							"#                                 ADD_MONTHS(to_date(Calendar.Date),-12) AS PreviousYearDate,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'y'),date_format(Calendar.Date, 'MM')) AS CalendarPeriod,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'EEEE'), \", \" ,date_format(Calendar.Date, 'MMMM'), \" \"  , date_format(Calendar.Date, 'd'),  \" \" , date_format(Calendar.Date, 'y')) AS DateName,\r\n",
							"#                                 date_format(Calendar.Date, 'y') AS CalendarYear,\r\n",
							"#                                 CONCAT(\"Year \", date_format(Calendar.Date, 'y')) AS CalendarYearName,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'y'), '-Q', date_format(Calendar.Date, 'q')) AS CalendarQuarter,\r\n",
							"#                                 CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterName,\r\n",
							"#                                 RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2) AS CalendarMonth,\r\n",
							"#                                 date_format(Calendar.Date, 'MMMM') AS MonthShortName,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'MMMM'), \", \", date_format(Calendar.Date, 'y') ) AS CalendarMonthName,\r\n",
							"#                                 RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2) AS CalendarWeek,\r\n",
							"#                                 CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \",date_format(Calendar.Date, 'y')) AS CalendarWeekName,\r\n",
							"#                                 RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3) AS CalendarDayOfYear,\r\n",
							"#                                 CONCAT(\"Day \", RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3), \", \",date_format(Calendar.Date, 'y')) AS CalendarDayOfYearName,\r\n",
							"#                                 RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2) AS DayOfMonth,\r\n",
							"#                                 CONCAT(\"Day \", RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2)) AS DayOfMonthName,\r\n",
							"#                                 dayofweek(Calendar.Date) AS DayOfWeek,\r\n",
							"#                                 CONCAT(\"Day \",dayofweek(Calendar.Date)) AS DayOfWeekName,\r\n",
							"#                                 date_format(Calendar.Date, 'EEEE') AS Weekday,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'y'),', ',RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2)) AS CalendarWeekOfYear,\r\n",
							"#                                 CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \" ,date_format(Calendar.Date, 'y')) AS CalendarWeekOfYearName,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'y'), \", \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2)) AS CalendarMonthOfYear,\r\n",
							"#                                 CONCAT(\"Month \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2), \", \",date_format(Calendar.Date, 'y') ) AS CalendarMonthOfYearName,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'y'), \", \", date_format(Calendar.Date, 'q')) AS CalendarQuarterOfYear,\r\n",
							"#                                 CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterOfYearName,\r\n",
							"#                                 CASE \r\n",
							"#                                     WHEN int(date_format(Calendar.Date, 'q')) < 3\r\n",
							"#                                     THEN CONCAT(date_format(Calendar.Date, 'y'), '-H1')\r\n",
							"#                                     ELSE CONCAT(date_format(Calendar.Date, 'y'), '-H2')\r\n",
							"#                                 END AS CalendarHalfYear,\r\n",
							"#                                 CONCAT(RIGHT(Calendar.FiscalYear,4), right(CONCAT('0',FPO.FiscalMonth),2)) AS FiscalPeriod,\r\n",
							"#                                 RIGHT(Calendar.FiscalYear,4) AS FiscalYear,\r\n",
							"#                                 CONCAT(\"Fiscal Year \", RIGHT(Calendar.FiscalYear,4)) AS FiscalYearName,\r\n",
							"#                                 CONCAT(RIGHT(Calendar.FiscalYear,4), '-Q', FPO.FiscalQuarter ) AS FiscalQuarter,\r\n",
							"#                                 CONCAT(\"Fiscal Quarter \", FPO.FiscalQuarter, \", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalQuarterName,\r\n",
							"#                                 RIGHT(CONCAT('0', FPO.FiscalMonth),2) AS FiscalMonth,\r\n",
							"#                                 CONCAT(date_format(Calendar.Date, 'MMMM'),\", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalMonthName,\r\n",
							"#                                 CASE \r\n",
							"#                                     WHEN FPO.FiscalQuarter < 3\r\n",
							"#                                     THEN CONCAT(RIGHT(Calendar.FiscalYear,4), '-H1')\r\n",
							"#                                     ELSE CONCAT(RIGHT(Calendar.FiscalYear,4), '-H2')\r\n",
							"#                                 END AS FiscalHalfYear,\r\n",
							"#                                 Calendar.Calendar AS Calendar\r\n",
							"#                         FROM \r\n",
							"#                             (SELECT CONCAT(FPI.CALENDAR, '_' ,FD.value) AS DateCode,\r\n",
							"#                                     FPI.CALENDAR,\r\n",
							"#                                     FPI.FISCALYEAR,\r\n",
							"#                                     FD.value AS Date\r\n",
							"#                              FROM TempTable_FiscalPeriodAdditionalDates FPI\r\n",
							"#                              CROSS JOIN FiscalDates FD\r\n",
							"#                              WHERE FD.Value BETWEEN FPI.FiscalPeriodStartDate AND FPI.FiscalPeriodEndDate) AS Calendar\r\n",
							"#                              LEFT JOIN TempTable_FiscalPeriodAdditionalDates FPO\r\n",
							"#                                   ON Calendar.Date BETWEEN FPO.FiscalPeriodStartDate AND FPO.FiscalPeriodEndDate\r\n",
							"#                                   AND Calendar.Calendar = FPO.Calendar\r\n",
							"#                         \"\"\")\r\n",
							"# #Write cleansed data\r\n",
							"# DF_Calendar.write.format(\"delta\")\\\r\n",
							"#                  .mode(\"overwrite\")\\\r\n",
							"#                  .option(\"overwriteSchema\", \"false\")\\\r\n",
							"#                  .save(CuratedADLSPath)\r\n",
							"\r\n",
							"# #Display Dataframe Result\r\n",
							"# if Debug == True:\r\n",
							"#     print(\"Total Records: \" + str(DF_Calendar.count()))\r\n",
							"#     display(DF_Calendar.take(NoOfRecordsToDisplay))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Curated Target Folder Path\r\n",
							"CuratedADLSPath = 'abfss://%s@%s.dfs.core.windows.net/%s' % (CleansedContainerName, CleansedStorageAccountName, CleansedRelativePath +\"/\" + \"Dates\")\r\n",
							"\r\n",
							"DF_StartEndDates =spark.sql(f\"\"\"SELECT CAST(\"{GeneratedDatesStartDate}\" AS Date) FiscalStartDate, CAST(MAX(FiscalPeriodEndDate) AS Date)+1 AS FiscalEndDate\r\n",
							"                                FROM TempTable_FiscalPeriod\"\"\")\r\n",
							"\r\n",
							"#Create List from DF_StartEndDates\r\n",
							"DatesCollection = DF_StartEndDates.collect()\r\n",
							"\r\n",
							"#Generate Date Ranges from Start & End Dates \r\n",
							"def DimDate_PeriodInDaysDateRange(start_date, end_date):\r\n",
							"    for n in range(int((end_date - start_date).days)):\r\n",
							"        yield start_date + timedelta(n)\r\n",
							"\r\n",
							"#Initialize list to store dates between Start & End Date\r\n",
							"List_DimDatePeriodInDaysDates = []\r\n",
							"\r\n",
							"#Loop in DatesCollection and generate all days between Start & End Date\r\n",
							"for row in DatesCollection:\r\n",
							"    PeriodInDaysStartDate = row[\"FiscalStartDate\"]\r\n",
							"    PeriodInDaysEndDate =  row[\"FiscalEndDate\"]\r\n",
							"    for single_date in DimDate_PeriodInDaysDateRange(PeriodInDaysStartDate, PeriodInDaysEndDate):\r\n",
							"        #Append single days to Period In Days Dates List\r\n",
							"        List_DimDatePeriodInDaysDates.append(single_date.strftime(\"%Y-%m-%d\"))\r\n",
							"\r\n",
							"#Define the Dataframe from Period In Days Dates List\r\n",
							"DF_PeriodInDaysDates = spark.createDataFrame(List_DimDatePeriodInDaysDates, StringType())\r\n",
							"DF_PeriodInDaysDates.createOrReplaceTempView('FiscalDates')\r\n",
							"\r\n",
							"#Define the Dataframe\r\n",
							"GenDates = spark.sql(f\"\"\"SELECT FiscalPeriodStartDate,\r\n",
							"                               FiscalPeriodEndDate,\r\n",
							"                               Calendar,\r\n",
							"                               FiscalYear,\r\n",
							"                               PeriodName,\r\n",
							"                               FiscalMonth,\r\n",
							"                               FiscalQuarter,\r\n",
							"                               Type, \r\n",
							"                               SyncStartDateTime\r\n",
							"                        FROM TempTable_FiscalPeriod\r\n",
							"                        UNION ALL\r\n",
							"                        (SELECT distinct(date_add(last_day(add_months(Date(FD.value), -1 )),1)) FiscalPeriodStartDate,\r\n",
							"                               LAST_DAY(FD.value) FiscalPeriodEndDate,\r\n",
							"                               '{HistoricalCalendar}' AS Calendar,\r\n",
							"                               CASE \r\n",
							"                                    WHEN month(FD.value) BETWEEN 7 AND 12 \r\n",
							"                                        THEN CONCAT('FY', year(FD.value) +1)\r\n",
							"                                    ELSE CONCAT('FY', year(FD.value)) \r\n",
							"                               END AS FiscalYear,\r\n",
							"\r\n",
							"                               CASE \r\n",
							"                                    WHEN month(FD.value) BETWEEN 7 AND 12\r\n",
							"                                        THEN CONCAT('Periode ', month(FD.value) - 6)\r\n",
							"                                    ELSE CONCAT('Periode ', month(FD.value) + 6)\r\n",
							"                               END AS PeriodName,\r\n",
							"\r\n",
							"                               CASE \r\n",
							"                                    WHEN month(FD.value) BETWEEN 7 AND 12\r\n",
							"                                        THEN month(FD.value) - 6\r\n",
							"                                    ELSE month(FD.value) + 6\r\n",
							"                               END AS FiscalMonth,\r\n",
							"                               CASE \r\n",
							"                                    WHEN month(FD.value) BETWEEN 7 AND 9 \r\n",
							"                                        THEN 1\r\n",
							"                                    WHEN month(FD.value) BETWEEN 10 AND 12\r\n",
							"                                        THEN 2\r\n",
							"                                    WHEN month(FD.value) BETWEEN 1 AND 3\r\n",
							"                                        THEN 3\r\n",
							"                                    WHEN month(FD.value) BETWEEN 4 AND 6\r\n",
							"                                        THEN 4\r\n",
							"                               END AS FiscalQuarter,\r\n",
							"                               1 AS Type,\r\n",
							"                               current_timestamp() as SyncStartDateTime                               \r\n",
							"                               FROM FiscalDates AS FD\r\n",
							"                               WHERE FD.value < (Select MIN(FiscalPeriodStartDate) from TempTable_FiscalPeriod WHERE Calendar = '{HistoricalCalendar}'))\r\n",
							"                               \"\"\")\r\n",
							"\r\n",
							"#Register the Temp Table\r\n",
							"GenDates.createOrReplaceTempView(\"TempTable_FiscalPeriodAdditionalDates\")\r\n",
							"\r\n",
							"#Define the Dataframe\r\n",
							"DF_Calendar = spark.sql(f\"\"\"SELECT  \r\n",
							"                                '-1' AS DateCode,\r\n",
							"                                to_date('1900-01-01') AS Date,\r\n",
							"                                to_date('1989-01-01') AS PreviousYearDate,\r\n",
							"                                '190001' AS CalendarPeriod,\r\n",
							"                                'Monday, January 1 1900' AS DateName,\r\n",
							"                                '1900' AS CalendarYear,\r\n",
							"                                'Year 1900' AS CalendarYearName,\r\n",
							"                                '1900-Q1' AS CalendarQuarter,\r\n",
							"                                'Quarter 1, 1900' AS CalendarQuarterName,\r\n",
							"                                '01' AS CalendarMonth,\r\n",
							"                                'January' AS MonthShortName,\r\n",
							"                                'January, 1900' AS CalendarMonthName,\r\n",
							"                                '01' AS CalendarWeek,\r\n",
							"                                'Week 01, 1900' AS CalendarWeekName,\r\n",
							"                                '001' AS CalendarDayOfYear,\r\n",
							"                                'Day 001, 1900' AS CalendarDayOfYearName,\r\n",
							"                                '01' AS DayOfMonth,\r\n",
							"                                'Day 01' AS DayOfMonthName,\r\n",
							"                                '1' AS DayOfWeek,\r\n",
							"                                'Day 1' AS DayOfWeekName,\r\n",
							"                                'Monday' AS Weekday,\r\n",
							"                                '1900, 01' AS CalendarWeekOfYear,\r\n",
							"                                'Week 01, 1900' AS CalendarWeekOfYearName,\r\n",
							"                                '1900, 01' AS CalendarMonthOfYear,\r\n",
							"                                'Month 01, 1900' AS CalendarMonthOfYearName,\r\n",
							"                                '1900, 1' AS CalendarQuarterOfYear,\r\n",
							"                                'Quarter 1, 1900' AS CalendarQuarterOfYearName,\r\n",
							"                                '1900-H1' AS CalendarHalfYear,\r\n",
							"                                '190001' AS FiscalPeriod,\r\n",
							"                                '1900' AS FiscalYear,\r\n",
							"                                'Fiscal Year 1900' AS FiscalYearName,\r\n",
							"                                '1900-Q1' AS FiscalQuarter,\r\n",
							"                                'Fiscal Quarter 1, 1900' AS FiscalQuarterName,\r\n",
							"                                '01' AS FiscalMonth,\r\n",
							"                                'January, 1900' AS FiscalMonthName,\r\n",
							"                                '1900-H1' AS FiscalHalfYear,\r\n",
							"                                '' AS Calendar\r\n",
							"                        UNION ALL\r\n",
							"                        SELECT  Calendar.DateCode,\r\n",
							"                                to_date(Calendar.Date) AS Date,\r\n",
							"                                ADD_MONTHS(to_date(Calendar.Date),-12) AS PreviousYearDate,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'),date_format(Calendar.Date, 'MM')) AS CalendarPeriod,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'EEEE'), \", \" ,date_format(Calendar.Date, 'MMMM'), \" \"  , date_format(Calendar.Date, 'd'),  \" \" , date_format(Calendar.Date, 'y')) AS DateName,\r\n",
							"                                date_format(Calendar.Date, 'y') AS CalendarYear,\r\n",
							"                                CONCAT(\"Year \", date_format(Calendar.Date, 'y')) AS CalendarYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), '-Q', date_format(Calendar.Date, 'q')) AS CalendarQuarter,\r\n",
							"                                CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterName,\r\n",
							"                                RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2) AS CalendarMonth,\r\n",
							"                                date_format(Calendar.Date, 'MMMM') AS MonthShortName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'MMMM'), \", \", date_format(Calendar.Date, 'y') ) AS CalendarMonthName,\r\n",
							"                                RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2) AS CalendarWeek,\r\n",
							"                                CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \",date_format(Calendar.Date, 'y')) AS CalendarWeekName,\r\n",
							"                                RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3) AS CalendarDayOfYear,\r\n",
							"                                CONCAT(\"Day \", RIGHT(CONCAT('00', date_format(Calendar.Date, 'D')),3), \", \",date_format(Calendar.Date, 'y')) AS CalendarDayOfYearName,\r\n",
							"                                RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2) AS DayOfMonth,\r\n",
							"                                CONCAT(\"Day \", RIGHT(CONCAT('0', date_format(Calendar.Date, 'd')),2)) AS DayOfMonthName,\r\n",
							"                                dayofweek(Calendar.Date) AS DayOfWeek,\r\n",
							"                                CONCAT(\"Day \",dayofweek(Calendar.Date)) AS DayOfWeekName,\r\n",
							"                                date_format(Calendar.Date, 'EEEE') AS Weekday,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'),', ',RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2)) AS CalendarWeekOfYear,\r\n",
							"                                CONCAT(\"Week \", RIGHT(CONCAT('0', weekofyear(Calendar.Date)),2), \", \" ,date_format(Calendar.Date, 'y')) AS CalendarWeekOfYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), \", \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2)) AS CalendarMonthOfYear,\r\n",
							"                                CONCAT(\"Month \", RIGHT(CONCAT('0',date_format(Calendar.Date, 'M')),2), \", \",date_format(Calendar.Date, 'y') ) AS CalendarMonthOfYearName,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'y'), \", \", date_format(Calendar.Date, 'q')) AS CalendarQuarterOfYear,\r\n",
							"                                CONCAT(\"Quarter \", date_format(Calendar.Date, 'q'), \", \", date_format(Calendar.Date, 'y')) AS CalendarQuarterOfYearName,\r\n",
							"                                CASE \r\n",
							"                                    WHEN int(date_format(Calendar.Date, 'q')) < 3\r\n",
							"                                    THEN CONCAT(date_format(Calendar.Date, 'y'), '-H1')\r\n",
							"                                    ELSE CONCAT(date_format(Calendar.Date, 'y'), '-H2')\r\n",
							"                                END AS CalendarHalfYear,\r\n",
							"                                CONCAT(RIGHT(Calendar.FiscalYear,4), right(CONCAT('0',FPO.FiscalMonth),2)) AS FiscalPeriod,\r\n",
							"                                RIGHT(Calendar.FiscalYear,4) AS FiscalYear,\r\n",
							"                                CONCAT(\"Fiscal Year \", RIGHT(Calendar.FiscalYear,4)) AS FiscalYearName,\r\n",
							"                                CONCAT(RIGHT(Calendar.FiscalYear,4), '-Q', FPO.FiscalQuarter ) AS FiscalQuarter,\r\n",
							"                                CONCAT(\"Fiscal Quarter \", FPO.FiscalQuarter, \", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalQuarterName,\r\n",
							"                                RIGHT(CONCAT('0', FPO.FiscalMonth),2) AS FiscalMonth,\r\n",
							"                                CONCAT(date_format(Calendar.Date, 'MMMM'),\", \", RIGHT(Calendar.FiscalYear,4)) AS FiscalMonthName,\r\n",
							"                                CASE \r\n",
							"                                    WHEN FPO.FiscalQuarter < 3\r\n",
							"                                    THEN CONCAT(RIGHT(Calendar.FiscalYear,4), '-H1')\r\n",
							"                                    ELSE CONCAT(RIGHT(Calendar.FiscalYear,4), '-H2')\r\n",
							"                                END AS FiscalHalfYear,\r\n",
							"                                Calendar.Calendar AS Calendar\r\n",
							"                        FROM \r\n",
							"                            (SELECT CONCAT(FPI.CALENDAR, '_' ,FD.value) AS DateCode,\r\n",
							"                                    FPI.CALENDAR,\r\n",
							"                                    FPI.FISCALYEAR,\r\n",
							"                                    FD.value AS Date\r\n",
							"                             FROM TempTable_FiscalPeriodAdditionalDates FPI\r\n",
							"                             CROSS JOIN FiscalDates FD\r\n",
							"                             WHERE FD.Value BETWEEN FPI.FiscalPeriodStartDate AND FPI.FiscalPeriodEndDate) AS Calendar\r\n",
							"                             LEFT JOIN TempTable_FiscalPeriodAdditionalDates FPO\r\n",
							"                                  ON Calendar.Date BETWEEN FPO.FiscalPeriodStartDate AND FPO.FiscalPeriodEndDate\r\n",
							"                                  AND Calendar.Calendar = FPO.Calendar\r\n",
							"                        \"\"\")\r\n",
							"\r\n",
							"#Write cleansed data\r\n",
							"DF_Calendar.write.format(\"delta\")\\\r\n",
							"                 .mode(\"overwrite\")\\\r\n",
							"                 .option(\"overwriteSchema\", \"false\")\\\r\n",
							"                 .save(CuratedADLSPath)\r\n",
							"\r\n",
							"#Display Dataframe Result\r\n",
							"if Debug == True:\r\n",
							"    print(\"Total Records: \" + str(DF_Calendar.count()))\r\n",
							"    display(DF_Calendar.take(NoOfRecordsToDisplay))\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Test')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "Sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6a404cdf-74ad-4309-8fde-c4a2d303f7a0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/35c7c256-febc-4c53-82a5-71e8573a3c24/resourceGroups/rg-pmishra-training-001/providers/Microsoft.Synapse/workspaces/trainparmish/bigDataPools/Sparkpool",
						"name": "Sparkpool",
						"type": "Spark",
						"endpoint": "https://trainparmish.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" **#One to Many Relationship**"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import pandas as pd\r\n",
							"# Sample data\r\n",
							"orders_data = {'order_id': [1, 2, 3, 4],\r\n",
							"               'customer_id': [101, 102, 101, 103],\r\n",
							"               'order_date': ['2023-01-01', '2023-02-01', '2023-02-15', '2023-03-01']}\r\n",
							"order_details_data = {'order_id': [1, 1, 2, 3, 3, 3],\r\n",
							"                      'product_id': [201, 202, 203, 204, 205, 206],\r\n",
							"                      'quantity': [2, 1, 3, 1, 2, 2]}\r\n",
							"# Create DataFrame for orders and order details\r\n",
							"orders_df = pd.DataFrame(orders_data)\r\n",
							"print(orders_df)\r\n",
							"order_details_df = pd.DataFrame(order_details_data)\r\n",
							"print(order_details_df)\r\n",
							"# Merge DataFrames to create one-to-many relationship\r\n",
							"merged_df = pd.merge(orders_df, order_details_df, on='order_id')\r\n",
							"print(merged_df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"**Many-to-Many Relationship**"
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas as pd\r\n",
							"# Sample data\r\n",
							"students_data = {'student_id': [1, 2, 3, 4],\r\n",
							"                 'student_name': ['Alice', 'Bob', 'Charlie', 'David']}\r\n",
							"courses_data = {'course_id': [101, 102, 103],\r\n",
							"                'course_name': ['Math', 'Science', 'History']}\r\n",
							"student_courses_data = {'student_id': [1, 1, 2, 3, 3],\r\n",
							"                        'course_id': [101, 102, 101, 102, 103]}\r\n",
							"# Create DataFrame for students, courses, and student courses\r\n",
							"students_df = pd.DataFrame(students_data)\r\n",
							"print(students_df)\r\n",
							"courses_df = pd.DataFrame(courses_data)\r\n",
							"print(courses_df)\r\n",
							"student_courses_df = pd.DataFrame(student_courses_data)\r\n",
							"print(student_courses_df)\r\n",
							"# Merge DataFrames to create many-to-many relationship\r\n",
							"merged_df = pd.merge(student_courses_df, students_df, on='student_id')\r\n",
							"merged_df = pd.merge(merged_df, courses_df, on='course_id')\r\n",
							"print(merged_df)"
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Sparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "uksouth"
		}
	]
}